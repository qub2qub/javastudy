05 Building Blocks

почему нельзя сделать синхронизированный конструктор?
http://docs.oracle.com/javase/specs/jls/se8/html/jls-8.html#jls-8.8.3
http://stackoverflow.com/questions/4880168/why-cant-java-constructors-be-synchronized

Java theory and practice: Safe construction techniques
http://www.ibm.com/developerworks/java/library/j-jtp0618/index.html

можно ли добавить синхронизацию в интерфейс?
НЕТ.

Where practical, delegation is one of the most effective strategies for creating thread-safe classes: 
just let existing thread-safe classes manage all the state.

****5.1    Synchronized collections****
The synchronized collection classes include: 
1) Vector and Hashtable,
2) the synchronized wrapper classes created by the Collections.synchronizedXxx() factory methods.

Problems with synchronized collections
The synchronized collections are thread-safe, but you may sometimes need to use additional client-side locking to guard COMPOUND ACTIONS: 
iteration, navigation, and conditional operations such as "put-if-absent".

With a synchronized collection, these compound actions are still technically thread-safe even without client-side locking, but they may not behave as you might expect when other threads can concurrently modify the collection.
public class UnsafeVectorHelpers {
    public static Object getLast(Vector list) {
        int lastIndex = list.size() - 1;
        return list.get(lastIndex);
    }

    public static void deleteLast(Vector list) {
        int lastIndex = list.size() - 1;
        list.remove(lastIndex);
    }
}
Например, list.size() и list.remove() --- это 2 разных действия.
И если getLast() и deleteLast() наложатся друг на друга - может быть exception if asked for a nonexistent element.
Поэтому надо дополнительно делать client-side синхронизацию, где локом будет сам list\vector, но тогда любые операцию с этим листом может делать только 1 поток, что немного ухудшает эффективность многопоточности.

Because the synchronized collections commit to a synchronization policy that supports client-side locking, it is possible to create new operations that are atomic with respect to other collection operations as long as we know which lock to use. 

The synchronized collection classes guard each method with the lock on the synchronized collection object itself. By acquiring the collection lock we can make getLast() and deleteLast atomic, ensuring that the size of the Vector does not change between calling size and get, as shown in Listing 5.2.
synchronized (list) { ... }

The problem of unreliable iteration can again be addressed by client-side locking, at some additional cost to scalability. By holding the Vector lock for the duration of iteration, as shown in Listing 5.4, we prevent other threads from modifying the Vector while we are iterating it. Unfortunately, we also prevent other threads from accessing it at all during this time, impairing concurrency.

можно передать объект для синхронизации в Collections.synchronizedXxx(YYYY);

*** 5.1.2 Iterators and ConcurrentModificationException ***
FOR-EACH LOOP использует итератор в итоге: Internally, javac generates code that uses an Iterator, repeatedly calling hasNext() and next() to iterate the List.

!!! using iterators DOES NOT OBVIATE ( НЕ устраняет необходимость) the need to lock the collection during iteration if other threads can concurrently modify it.

The iterators returned by the synchronized collections are not designed to deal with concurrent modification, and they are FAIL-FAST—meaning that if they detect that the collection has changed since iteration began, they throw the unchecked ConcurrentModificationException.

--- Iterators are implemented by associating a modification count with the collection: if the modification count changes during iteration, hasNext or next throws ConcurrentModificationException.
--- However, this check is done without synchronization, so there is a risk of seeing a stale value of the modification count and therefore that the iterator does not realize a modification has been made. This was a deliberate(преднамеренный, обдуманный компромисс дизайна) design trade-off to reduce the performance impact of the concurrent modification detection code.

Блокировать коллекцию на время итерации не всегда желательно, т.к. если она большая или операция над каждым элементом довольно долгая, то другим потокам придётся ждать довольно долго.
также есть риск возникновения starvation or deadlock. 
и также locking collections for significant periods of time hurts application scalability(масштабируемость, расширяемость). 
т.е. если много потоков будут ждать освобождения лока и бороться за него, то будут простаивать\неэффективно ис-ся ресурсы процессора и пропускная способность\производительность будет страдать.

Альтернативой для блокировки коллекции при итерации может быть клонирование коллекции и итерация по клону, но насколько это выгодный компромисс (клонирование имеет obvious performance cost) -- зависит от многих факторов: 
1) размер коллекции
2) сколько работы будет сделано над каждым элементом
3) относительная частота итерации по коллекции, в сравнении с другими операциями с коллекцией
4) требования отзывчивости (responsiveness)
5) требования пропускной способности.

*** 5.1.3 Hidden iterators СКРЫТЫЕ ИТЕРАТОРЫ ***
!!! You have to remember to use locking everywhere a shared collection might be iterated.

Скрытая итерации будет например при выводе коллекции в дебаг сообщение (HiddenIterator class), т.к. будет вызван StringBuilder.append(Object) и collection's toString method, а в коллекции для каждого её элемента будет вызван toString. И в итоге может быть ConcurrentModificationException. 
!!! поэтому вывод в дебаг тоже должен происходить с получение лока для этой коллекции (в соответствии с логикой блокировки).

Just as encapsulating an object’s state makes it easier to preserve its invariants, encapsulating its synchronization makes it easier to enforce its synchronization policy.
>>> encapsulating its synchronization: когда синхронизация уже сделана внутри класса/коллекции и автоматически применяется при операциях

Проблема в том, что чем дальше расстояние между состоянием и блокировкой, которая охраняет это состояние, тем БОЛЬШЕ шанс что кто-то забудет сделать блокировку при работе с этим состоянием.
The real lesson here is that the greater the distance between the state and the synchronization that guards it, the more likely that someone will forget to use proper synchronization when accessing that state. 
If HiddenIterator wrapped the HashSet with a synchronizedSet, encapsulating the synchronization, this sort of error would not occur.

-Также итерация косвенно вызывается в:
1) hashCode() и equals() методами коллекции. Так может быть если коллекция является ключом или значением в другой коллекции.
2) containsAll, removeAll, and retainAll methods, 
3) as well as the constructors that take collections as arguments, also iterate the collection. 
Это всё может привести к ConcurrentModificationException.
4) toString()

*** 5.2 Concurrent collections ***
Synchronized collections achieve their thread safety by serializing all access to the collection's state. The cost of this approach is poor concurrency; when multiple threads contend for the collection-wide lock, throughput suffers.

The new concurrent collections, on the other hand, are designed for concurrent access from multiple threads.
Replacing synchronized collections with concurrent collections can offer dramatic scalability improvements with little risk.

1) CopyOnWriteArrayList for cases where traversal(проход по всем элементам) is the dominant operation.
Пример: набор event listener-ов, событий больше и чаще, чем когда мы добавляем/удаляем listeners.
2) The new ConcurrentMap interface adds support for common compound actions such as put-if-absent, replace, and conditional remove.
----------------
Java 5.0 also adds two new collection types, Queue and BlockingQueue. 

1) A Queue is intended to hold a set of elements temporarily while they await processing. 
Several implementations are provided:
[1.A] ConcurrentLinkedQueue, a traditional FIFO queue,
[1.B] PriorityQueue, a (NON CONCURRENT) priority ordered queue. 
QUEUE OPERATIONS DO NOT BLOCK; IF THE QUEUE IS EMPTY, THE RETRIEVAL OPERATION RETURNS NULL. 
While you can simulate the behavior of a Queue with a List — in fact, LinkedList also implements Queue — the Queue classes were added because eliminating the random-access requirements of List admits more efficient concurrent implementations.

@Javadoc for PriorityQueue:
/* Priority queue represented as a balanced binary heap: 
the two children of queue[n] are queue[2*n+1] and queue[2*n+2] (in a zero-based array).  
The priority queue is ordered by comparator, or by the elements' natural ordering (if comparator is null).
For each node n in the heap and each descendant d of n, n <= d.  The element with the lowest value is in queue[0], assuming the queue is nonempty. */

** Что за RANDOM ACCESS requirements для List?
когда по индексу можно взять ЛЮБОЙ элемент из листа за ПОСТОЯННОЕ время.

2) BlockingQueue extends Queue to add BLOCKING INSERTION AND RETRIEVAL OPERATIONS. 
If the queue is empty, a retrieval blocks until an element is available, and if the queue is full (for bounded queues) an insertion blocks until there is space available. Blocking queues are extremely useful in producer-consumer designs, and are covered in greater detail in Section 5.3.
---------------
Just as ConcurrentHashMap is a concurrent replacement for a synchronized hash-based Map, 
Java 6 adds ConcurrentSkipListMap and ConcurrentSkipListSet, which are concurrent replacements for a synchronized SortedMap or SortedSet (such as TreeMap or TreeSet wrapped with synchronizedMap).
Т.е. ConcurrentSkipListMap and ConcurrentSkipListSet ис-ся для замены TreeMap or TreeSet, обёрнутых в synchronizedMap метод.
------------------------------------------

*** 5.2.1 ConcurrentHashMap ***

THE SYNCHRONIZED COLLECTIONS CLASSES HOLD A LOCK FOR THE DURATION OF EACH OPERATION. 
Some operations, such as HashMap.get() or List.contains(), may involve more work than is initially obvious: 
ЧТОБЫ пройти по всем hash bucket or list и найти нужный объект надо вызывать .equals() для всех проверяемых кандидатов (equals сам по себе может повлечь большое кол-во вычислений).
Или в hash-based collection при плохом методе hashCode() элементы неравномерно распределятся по бакетам (в самом плохом случае это станет LinkedList).

Поэтому ConcurrentHashMap ис-ет другой механизм блокировки:
it uses an entirely different locking strategy that offers better concurrency and scalability. 
Instead of synchronizing every method on a common lock, restricting access to a single thread at a time, it uses a FINER-GRAINED LOCKING MECHANISM called LOCK STRIPING (череда локов, т.е. коллекция как бы делится на отдельные блоки и для каждого будет свой лок) to allow a greater degree of shared access.
Элементы делятся на группы и на каждую такую группу будет отдельный лок.

Arbitrarily many READING THREADS can access the map concurrently, READERS can access the map concurrently with WRITERS, and a limited NUMBER OF WRITERS can modify the map concurrently. The result is far higher throughput under concurrent access, with little performance penalty for single-threaded access.
--------
Ещё одно улучшение в новых concurrent collections:
они  предоставляют новый итератор, который не выбрасывает ConcurrentModificationException, thus eliminating the need to lock the collection during iteration.

The iterators returned by ConcurrentHashMap are WEAKLY CONSISTENT instead of FAIL-FAST.
1) FAIL-FAST iterator — meaning that if they detect that the collection has changed since iteration began, they throw the unchecked ConcurrentModificationException.
2) WEAKLY CONSISTENT iterator -- can tolerate (терпеть, выносить, допускать, дозволять) concurrent modification, traverses elements as they existed when the iterator was constructed, and may (but is not guaranteed to) reflect modifications to the collection after the construction of the iterator.

Но в новых коллекциях пошли также на ряд компромиссов:
1) size() and isEmpty(), have been slightly weakened to reflect the concurrent nature of the collection (потому что эти 2 метода far less useful in concurrent environments, потому что значения этих методов -- это постоянно движущиеся мишени, т.е. они и так постоянно меняются и нет смысла на них полагаться). Поэтому их ослабили, чтобы улучшить производительность других более важных методов, например, get, put, containsKey, and remove.
[1.A] size() is allowed to return an approximation instead of an exact count.
>>> т.е. показывает приблизительное кол-во элементов.

Единственной полезной фичей у старого способа (synchronized Map) осталась возможность залочить мэп для эксклюзивного(единственного) доступа. Это может пригодиться когда:
This might be necessary in unusual cases such as adding several mappings atomically, 
or iterating the Map several times and needing to see the same elements in the same order.

Since a ConcurrentHashMap cannot be locked for exclusive access, we cannot use client-side locking to create new atomic operations such as put-if-absent. 
Instead, a number of common compound operations such as put-if-absent, remove-if-equal, and replace-if-equal are implemented as atomic operations and specified by the ConcurrentMap interface.

*** 5.2.3    CopyOnWriteArrayList *** CopyOnWriteArraySet
CopyOnWriteArrayList is a concurrent replacement for a synchronized List that offers better concurrency in some common situations and eliminates the need to lock or copy the collection during iteration. (Similarly, CopyOnWriteArraySet is a concurrent replacement for a synchronized Set.)
Similarly, CopyOnWriteArraySet is a concurrent replacement for a synchronized Set.)

Условия потокобезопасности для effectively immutable объектов:
as long as an effectively immutable object is properly published, no further synchronization is required when accessing it. 

На этом факте и основывается CopyOnWriteArrayList:
The copy-on-write collections derive their thread safety from the fact that as long as an EFFECTIVELY IMMUTABLE OBJECT IS PROPERLY PUBLISHED, NO FURTHER SYNCHRONIZATION IS REQUIRED WHEN ACCESSING IT. 
They implement mutability by creating and republishing a new copy of the collection every time it is modified. Iterators for the copy-on-write collections retain a reference to the backing array that was current at the start of iteration, and since this will never change, they need to synchronize only briefly to ensure visibility of the array contents.

ITERATORS for the copy-on-write collections retain a reference to the backing array that was current at the start of iteration, and since this will never change, they need to synchronize only briefly to ensure visibility of the array contents. (т.е. убедиться только что ссылка на новую коллекцию было safely published ?)

As a result, multiple threads can iterate the collection without interference from one another or from threads wanting to modify the collection. 
The iterators returned by the copy-on-write collections do not throw ConcurrentModificationException and return the elements exactly as they were at the time the iterator was created, regardless of subsequent modifications.
** The returned iterator provides a snapshot of the state of the list when the iterator was constructed. No synchronization is needed while traversing the iterator. The iterator does NOT support the .remove() method.

-- Тут возникают проблемы с производительностью, т.к. при записи в лист надо делать его копию ( особенно если размер листа достаточно большой). 
Поэтому COPY-ON-WRITE COLLECTIONS выгодно использовать только когда чаще всего происходит итерация/чтение по коллекции, чем изменение коллекции.
!! the copy-on-write collections are reasonable to use only when iteration is far more common than modification. 

Поэтому COPY-ON-WRITE коллекции подходят для EVENT-NOTIFICATION SYSTEMS: 
когда приходят много событий и надо пробегать по списку листенеров чтобы найти обработчик. И при этом чаще надо обрабатывать события, чем удалять и добавлять обработчиков.
This criterion exactly describes many event-notification systems: 
delivering a notification requires iterating the list of registered listeners and calling each one of them, and in most cases registering or unregistering an event listener is far less common than receiving an event notification.

*** 5.3 Blocking queues and the producer-consumer pattern ***
Blocking queues provide blocking PUT and TAKE methods as well as the timed equivalents OFFER and POLL. 
If the queue is full,  X.put()  BLOCKS until space becomes available; 
if the queue is empty, X.take() BLOCKS until an element is available. 
Queues can be BOUNDED OR UNBOUNDED; unbounded queues are never full, so a put on an unbounded queue never blocks.

Blocking queues support the PRODUCER-CONSUMER DESIGN PATTERN. 
A producer-consumer design separates the identification(отождествление самой работы с выполнением этой работы) of work to be done from the execution of that work by placing work items on a "to do" list for later processing, rather than processing them immediately as they are identified. 
The producer-consumer pattern simplifies development because it removes code dependencies between producer and consumer classes, and simplifies workload management by DECOUPLING ACTIVITIES that may produce or consume data at different or variable rates.

One of the most common producer-consumer designs is a THREAD POOL COUPLED WITH A WORK QUEUE; this pattern is embodied(воплощён) in the Executor task execution framework that is the subject of Chapters 6 and 8.

Blocking queues also provide an "offer()" method, which returns a failure status if the item cannot be enqueued. This enables you to create more flexible policies for dealing with overload, such as shedding load, serializing excess work items and writing them to disk, reducing the number of producer threads, or throttling producers in some other manner.

>> add() - бросает exception, 
offer() - возвращает true/false, 
put() - wait + exception
>> take() - wait, 
poll() - waitTime/null

Bounded queues are a powerful resource management tool for building RELIABLE APPLICATIONS: they make your program more robust to overload by throttling activities that threaten to produce more work than can be handled.

While the producer-consumer pattern enables producer and consumer code to be decoupled from each other, their BEHAVIOR is still COUPLED indirectly through the SHARED WORK QUEUE.
It is tempting to assume that the consumers will always keep up, so that you need not place any bounds on the size of work queues, but this is a prescription for rearchitecting your system later. 
Build resource management into your design early using blocking queues — IT IS A LOT EASIER TO DO THIS UP FRONT THAN TO RETROFIT IT LATER. Blocking queues make this easy for a number of situations, but if blocking queues don't fit easily into your design, you can create other blocking data structures using Semaphore (see Section 5.5.3).

The class library contains several implementations of BlockingQueue:
1,2) LinkedBlockingQueue and ArrayBlockingQueue are FIFO queues, analogous to LinkedList and ArrayList but with better concurrent performance than a synchronized List. 
3) PriorityBlockingQueue is a priority-ordered queue, which is useful when you want to process elements in an order other than FIFO. Just like other sorted collections, PriorityBlockingQueue can compare elements according to their natural order (if they implement Comparable) or using a Comparator.
4) SynchronousQueue, is not really a queue at all, in that it maintains no storage space for queued elements. Instead, it maintains a list of queued threads waiting to enqueue or dequeue an element. In the dish-washing analogy, this would be like having no dish rack, but instead handing the washed dishes directly to the next available dryer.

-В SynchronousQueue работа по завершению сразу передаётся к потребителю, что гораздо надёжнее, чем сначала положить её в очередь, т.к. неизвестно когда её потом обработают(а тут производитель сразу знает что его результат пошёл обрабатываться дальше). Заодно, не тратятся ресурсы, чтобы положить её в очередь и забрать оттуда. Но при SynchronousQueue всегда должен быть хотя бы 1 свободный потребитель, чтобы ему передать результаты работы.
-Since a SynchronousQueue has no storage capacity, put() and take() will block unless another thread is already waiting to participate in the handoff. Synchronous queues are generally suitable only when there are enough consumers that there nearly always will be one ready to take the HANDOFF.

--Priority queue represented as a balanced binary heap.
** почитать более подробно про эту структуру
*******************************************************************
A heap is not a sorted structure and can be regarded as partially ordered.
можно сделать heap как tree или как array.
A common implementation of a heap is the binary heap, in which the tree is a COMPLETE BINARY TREE.
binary tree -- значит, что у каждого нода есть 2 ребёнка.
A binary heap is a heap data structure that takes the form of a binary tree. 
Binary heaps are a common way of implementing priority queues.

и заполнять хип надо СВЕРХУ ВНИЗ и СПРАВА НАЛЕВО. Это необходимое условие, иначе заполнять нельзя.
и потом когда удалять элемент -- то на его место берётся самый правый нижний, и потом опускается на нужное место.
there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height — a heap with N nodes always has (log N) height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.

Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.

--heapify: create a heap out of given array of elements
--merge (union): joining two heaps to form a valid new heap containing all the elements of both, preserving the original heaps.
--meld: joining two heaps to form a valid new heap containing all the elements of both, destroying the original heaps.
***After an element is inserted into or deleted from a heap, the heap property may be violated and the heap must be balanced by internal operations.
**Balancing a heap is done by sift-up or sift-down operations (swapping elements which are out of order).

***A binary heap is defined as a binary tree with two additional constraints:
 1) Shape property: a binary heap is a complete binary tree; that is, all levels of the tree, except possibly the last one (deepest) are fully filled, and, if the last level of the tree is not complete, the nodes of that level are filled from left to right.
 2) Heap property: the key stored in each node is either greater than or equal to or less than or equal to the keys in the node's children, according to some total order.

*** Над кучей можно выполнять следующие операции:
1) Добавить элемент в кучу. Сложность O ( log ⁡ n ) 
2) Исключить максимальный элемент из кучи. Время работы O ( log ⁡ n ) 
3) Изменить значение любого элемента. Время работы O ( log ⁡ n )

На основе этих операций можно выполнять следующие действия:
4) Превратить неупорядоченный массив элементов в кучу. Сложность O ( n )
5) Отсортировать массив путём превращения его в кучу, а кучи в отсортированный массив. Время работы O ( n log ⁡ n )

**********************************************************************************
стэйбл сортировка -- относительный порядок равных элементов сохраняется
**********************************************************************************

5.3.1 Example: desktop search
FileCrawler class. (Producer and consumer tasks in a desktop search application.)
Producers and consumers can execute concurrently; if one is I/O-bound and the other is CPU-bound, executing them concurrently yields better overall throughput than executing them sequentially.
While this example uses explicitly managed threads, many producer-consumer designs can be expressed using the Executor task execution framework, which itself uses the producer-consumer pattern.

**** 5.3.2 Serial thread confinement **** (порядковый; последовательный)
The blocking queue implementations in java.util.concurrent all contain sufficient internal synchronization to safely publish objects from a producer thread to the consumer thread.

For mutable objects, producer-consumer designs and blocking queues facilitate(используют, продвигают) SERIAL THREAD CONFINEMENT for handing off ownership of objects from producers to consumers. 

---- A thread-confined object is owned EXCLUSIVELY BY A SINGLE THREAD, but that ownership CAN BE "TRANSFERRED" by publishing it safely where only one other thread will gain access to it and ensuring that the publishing thread does not access it after the handoff(пас, передача). The safe publication ensures that the object's state is visible to the new owner, and since the original owner will not touch it again, it is now confined to the new thread. The new owner may modify it freely since it has exclusive access.

---- Object pools exploit serial thread confinement, "lending" an object to a requesting thread. 
As long as the pool contains sufficient internal synchronization to publish the pooled object safely, and as long as the clients do not themselves publish the pooled object or use it after returning it to the pool, OWNERSHIP CAN BE TRANSFERRED SAFELY FROM THREAD TO THREAD.

---- One could also use other publication mechanisms for transferring ownership of a mutable object, but it is necessary to ensure that only one thread receives the object being handed off. Blocking queues make this easy; 

*** 5.3.3 Deques and work stealing ***
Java 6 also adds another two collection types, Deque (pronounced "deck") and BlockingDeque, that extend Queue and BlockingQueue. 
A DEQUE is a DOUBLE-ENDED QUEUE that allows efficient INSERTION AND REMOVAL FROM BOTH THE HEAD AND THE TAIL. Implementations include ArrayDeque and LinkedBlockingDeque. (bounded & unbounded)

- Just as blocking queues lend themselves(быть пригодным/подходящим для) to the producer-consumer pattern, 
- deques lend themselves to a related pattern called WORK STEALING. 

A producer-consumer design has one shared work queue for all consumers; 
IN A WORK STEALING DESIGN, EVERY CONSUMER HAS ITS OWN DEQUE. 
If a consumer exhausts the work in its own deque, it can steal work from the TAIL of someone else's deque. 
Work stealing can be more scalable than a traditional producer-consumer design because workers don't contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another's queue, it does so from the tail rather than the head, further reducing contention(соревнование, состязание).

*** 5.4 Blocking and interruptible methods ***
Threads may block, or pause, for several reasons: 
1) waiting for I/O completion, 
2) waiting to acquire a lock, 
3) waiting to wake up from Thread.sleep(), 
4) or waiting for the result of a computation in another thread. 

When a thread blocks, it is usually suspended and placed in one of the blocked thread states (BLOCKED(ждём монитора), WAITING(ждём вычисления), or TIMED_WAITING(вычисления или sleep)). 
The distinction between a blocking operation and an ordinary operation that (только, просто; единственно)merely takes a long time to finish is that a blocked thread must wait for an event that is beyond its control before it can proceed — the I/O completes, the lock becomes available, or the external computation finishes. When that external event occurs, the thread is placed back in the RUNNABLE state and becomes eligible again for scheduling.

The put() and take() methods of BlockingQueue throw the checked InterruptedException, as do a number of other library methods such as Thread.sleep(). 
When a method can throw InterruptedException, it is telling you that it is a blocking method, and further that if it is interrupted, IT WILL MAKE AN EFFORT TO STOP BLOCKING EARLY.

INTERRUPTION is a COOPERATIVE mechanism. (механизм сотрудничества)
One thread CANNOT FORCE ANOTHER TO STOP what it is doing and do something else; 
when thread "A" interrupts thread "B", "A" IS MERELY REQUESTING THAT "B" STOP WHAT IT IS DOING WHEN IT GETS TO A CONVENIENT STOPPING POINT — if it feels like it. 

While there is nothing in the API or language specification that demands any specific application-level semantics for interruption, the MOST SENSIBLE USE FOR INTERRUPTION IS TO CANCEL AN ACTIVITY. Blocking methods that are responsive to interruption make it easier to cancel long-running activities on a timely basis.

!!! When your code calls a method that throws InterruptedException, then YOUR METHOD IS A BLOCKING METHOD TOO, and MUST have a plan for RESPONDING TO INTERRUPTION. 

For library code, there are basically two choices:
1) PROPAGATE THE INTERRUPTED EXCEPTION. 
This is often the most sensible policy if you can get away with it — just propagate the InterruptedException to your caller. This could involve not catching InterruptedException, or catching it and throwing it again after performing some brief activity-specific cleanup.
2) RESTORE THE INTERRUPT. 
Sometimes you cannot throw InterruptedException, for instance when your code is part of a Runnable. In these situations, you must catch InterruptedException and restore the interrupted status by calling .interrupt() on the current thread, so that code higher up the call stack can see that an interrupt was issued, as demonstrated in Listing 5.10. [Thread.currentThread().interrupt();]
public class TaskRunnable implements Runnable {
    BlockingQueue<Task> blockingQueue;
    public void run() {
        try {
            processTask(blockingQueue.take());
        } catch (InterruptedException e) {
            // restore interrupted status
            Thread.currentThread().interrupt();
        }
    }
}

You can get much more sophisticated with interruption, but these two approaches should work in the vast majority of situations. But there is one thing you should NOT do with InterruptedException — [!! DO NOT] CATCH IT AND DO NOTHING IN RESPONSE. 
This deprives(лишать; отбирать, отнимать, не допускать) code higher up on the call stack of the opportunity to act on the interruption, because the evidence that the thread was interrupted is lost. 
The only situation in which it is acceptable to swallow an interrupt is when you are extending Thread and therefore control all the code higher up on the call stack. Cancellation and interruption are covered in greater detail in Chapter 7.

--------------------------------------------------------------------------------

*** 5.5 Synchronizers *** 07.01.2016 - 11.11.2018
A SYNCHRONIZER is any object that coordinates the control flow of threads based on its state. 
SYNCHRONIZER -- любой объект, который координирует ход выполнения потоков, в зависимости от своего состояния.

Типы\виды\разновидности синхронизаторов:
1) Blocking queues can act as synchronizers
2) semaphores,
3) barriers, 
4) latches,
5) FutureTask also acts like a latch. 
6) EXCHANGER -- another form of barrier.

All synchronizers share certain STRUCTURAL PROPERTIES: they encapsulate STATE that:
1) determines whether threads arriving at the synchronizer should be ALLOWED TO PASS or FORCED TO WAIT, 
2) provide methods to MANIPULATE that STATE, 
3) provide methods to WAIT EFFICIENTLY for the synchronizer TO ENTER the desired state. (Эффективно ждать момента, пока синхронизатор придёт в нужное состояние.)=поток ждёт, не расходуя ресурсы впустую.

*** 5.5.1 Latches (триггер, защёлка, клапан -- а ля шпингалет) 
*** CountDownLatch *** ОДНОРАЗОВЫЕ ЗАЩЁЛКИ
*** Как ОДНОРАЗОВЫЙ ЗАМОК НА ВОРОТАХ, чтобы открыть его -- надо ОПУСТИТЬ ВСЕ ЗУБЧИКИ (N-ое кол-во событий). -- но механизм "хрупкий", поэтому после 1 раза все зубчики сломались и больше он никого не сдержит --- *** ИЛИ как если бы чтобы открыть этот засов - надо собрать много пацанов, а потом ворота откроются и все бараны будут просто пробегать, и некому будет заново закрыть. *** Хотя на самом деле пацаны ждут что произойдёт определённое кол-во событий, после которых засов откроет ворота.

A latch is a synchronizer that can delay(задержать, приостановить) the progress of threads until it reaches its terminal state(конечное, последнее, заключительное состояние).

LATCHES ARE SINGLE-USE OBJECTS; 
ONCE A LATCH ENTERS THE TERMINAL STATE, IT CANNOT BE RESET.

A latch acts as a GATE: until the latch reaches the terminal state the gate is closed and no thread can pass, and in the terminal state the gate opens, allowing all threads to pass. 
ONCE THE LATCH REACHES THE TERMINAL STATE, IT CANNOT CHANGE STATE AGAIN, SO IT REMAINS OPEN FOREVER.

Latches can be used to ensure that certain activities do not proceed until other one-time activities complete, such as:
1) Ensuring that a computation does not proceed until resources it needs have been initialized.
2) Ensuring that a service does not start until other services on which it depends have started.
3) Waiting until all the parties involved in an activity, for instance the players in a multi-player game, are ready to proceed.

CountDownLatch is a flexible latch implementation that can be used in any of these situations; 
it allows one or more threads to wait for a set of events to occur. 

The latch state consists of a counter initialized to a positive number, representing the number of events to wait for.

The .countDown() method decrements the counter, indicating that an event has occurred, and the .await() methods wait for the counter to reach zero, which happens when all the events have occurred. If the counter is nonzero on entry, await() blocks until the counter reaches zero, the waiting thread is interrupted, or the wait times out.

TestHarness.JAVA -- Presumably, we wanted to measure how long it takes to run a task n times concurrently.
The first thing each worker thread does is wait on the starting gate; this ensures that none of them starts working until they all are ready to start. 
The last thing each does is count down on the ending gate; this allows the master thread to wait efficiently until the last of the worker threads has finished, so it can calculate the elapsed time.

*** 5.5.2 FutureTask ***
FutureTask also acts like a latch.
FutureTask implements Future, which describes an abstract result-bearing computation
(приносящее результат вычисление).

A Future represents the result of an asynchronous computation.

A computation represented by a FutureTask is implemented with a Callable, the result-bearing equivalent of Runnable, and can be in ONE OF THREE STATES: 
1) waiting to run, 
2) running, 
3) completed. 
Completion subsumes(включает) ALL THE WAYs a computation can complete, including normal completion, cancellation, and exception. 

ONCE A FUTURETASK ENTERS THE COMPLETED STATE, IT STAYS IN THAT STATE FOREVER.

The behavior of Future.get() depends on the state of the task: 
1) If it is completed, get returns the result immediately, 
2) and otherwise blocks until the task transitions to the completed state and then returns the result.
3) or throws an exception. 
FutureTask conveys(передаёт) the result from the thread executing the computation to the thread(s) retrieving the result; the specification of FutureTask guarantees that this transfer constitutes a safe publication of the result.

--- [eı`sıŋkrənəs] эйс-И-нкронэс ---
FutureTask is used by the Executor framework to represent:
1) asynchronous tasks
2) any potentially lengthy computation that can be started before the results are needed. 

Preloader.java -- creates a FutureTask that describes the task of loading product information from a database and a thread in which the computation will be performed. 
It provides a start method to start the thread, since it is inadvisable to start a thread from a constructor or static initializer. 
// private final FutureTask<ProductInfo> future = new FutureTask<>(() -> loadProductInfo());
When the program later needs the ProductInfo, it can call get(), which returns the loaded data if it is ready, or waits for the load to complete if not.

--Tasks described by Callable can THROW checked and unchecked exceptions, and any code can throw an Error. Whatever the task code may throw, it is wrapped in an ExecutionException and rethrown from Future.get(). 
--This complicates code that calls get(), not only because it must deal with the possibility of ExecutionException (and the unchecked CancellationException), but also because the cause of the ExecutionException is returned as a Throwable, which is inconvenient to deal with.

--When get() throws an ExecutionException in Preloader, the cause will fall into one of three categories: 
1) a checked exception thrown by the Callable, 
2) a RuntimeException, 
3) or an Error. 
We must handle each of these cases separately.

*** 5.5.3 Semaphores *** СЕМАФОРЫ *** (имеет постоянно изменяющееся состояние)
Семафоры бывают 
1) двоичные (binary semaphore) 
2) и подсчитывающие (counting semaphore),
3) а также именованные 
4) и неименованные.

Семафор обычно представляет собой целочисленную переменную или объект, над которыми разрешается производить три операции: ИНИЦИАЛИЗАЦИЮ, ИНКРЕМЕНТ и ДЕКРЕМЕНТ. 
В зависимости от того, как они определены, декремент может означать блокировку процесса или ресурса, а инкремент - его разблокировку. 

НАПРИМЕР:
 * Существует парковка по пропускам/разрешениям, которая одновременно может вмещать не более 5 автомобилей .
 * Если парковка заполнена полностью, то вновь прибывший автомобиль должен подождать пока не освободится хотя бы одно место/один пропуск. После этого он сможет припарковаться.
 * Заезжая на парковку, водила берёт пропуск у вахтёра, а при выезде возвращает пропуск.
 * ВАХТЁР с ограниченным числом пропусков --- это и есть СЕМАФОР.

// Устанавливаем флаг FAIR=true >> "справедливый", в таком случае метод aсquire() будет раздавать разрешения в порядке очереди (кто дольше ждёт - тот первый получит разрешение)

Counting semaphores are used to control the number of activities that can access a certain resource or perform a given action at the same time. 
Counting semaphores can be used to implement resource pools or to impose a bound(наложить ограничение) on a collection.

A Semaphore manages a set of VIRTUAL PERMITS; the initial number of permits is passed to the Semaphore constructor.
Activities can acquire permits (as long as some remain) and release permits when they are done with them. If no permit is available, .acquire() blocks until one is (пока хотя бы 1 разрешение не станет доступным) (or until interrupted or the operation times out).
The .release() method returns a permit to the semaphore.

ЛОГИЧЕСКИЕ ПЕРМИТЫ
---The implementation has no actual permit objects, and Semaphore does not associate dispensed permits with threads, so a permit acquired in one thread can be released from another thread. You can think of "acquire()" as consuming a permit and "release()" as creating one; a Semaphore is not limited to the number of permits it was created with. ---
A degenerate case of a counting semaphore is a binary semaphore, a Semaphore with an initial count of one. A binary semaphore can be used as a MUTEX(взаимное исключение) with nonreentrant locking semantics; whoever holds the sole permit holds the mutex.

ГДЕ ИСПОЛЬЗОВАТЬ:
Semaphores are useful for implementing RESOURCE POOLS such as database connection pools. 
While it is easy to construct a fixed-sized pool that fails if you request a resource from an empty pool, what you really want is to block if the pool is empty and unblock when it becomes nonempty again.

*** 5.5.4 Barriers *** ждёт всё кол-во участников и пропускает их через турникет. *** МНОГОРАЗОВЫЙ
*** BARRIERS как турникеты на скачках -- все лошади подтянулись (и неважно, без подковы или без наездника) -- турникет автоматически срабатывает на присутствие лошади и открывается когда все они появились, и сразу после этого закрывается и ждёт новую группу лошадей ***
*** а при LATCHES -- лошади может уже все и стоят у турникетов, но какой-то из них ещё подкову меняют, какая-то без наездника. И когда все эти подготовки завершатся - тогда турникеты откроются. Об этом судью должны уведомить смотрящие за каждой лошадью. ***

Barriers are similar to latches in that they block a group of threads until some event has occurred. 

!!! The key difference is that with a barrier, ALL THE THREADS MUST COME TOGETHER AT A BARRIER POINT AT THE SAME TIME IN ORDER TO PROCEED. 
!!! LATCHES ARE FOR WAITING FOR EVENTS; BARRIERS ARE FOR WAITING FOR OTHER THREADS.

A barrier implements the protocol some families use to rendezvous during a day at the mall: "Everyone meet at McDonald's at 6:00; once you get there, stay there until everyone shows up, and then we'll figure out what we're doing next."

- CyclicBarrier allows a fixed number of parties to rendezvous REPEATEDLY at a barrier point and is useful in PARALLEL ITERATIVE ALGORITHMS that break down a problem into a fixed number of independent subproblems.
-- Threads call .await() when they reach the barrier point, and "await" blocks until all the threads have reached the barrier point. If all threads meet at the barrier point, the barrier has been successfully passed, in which case all threads are released and the barrier is reset so it can be used again. 
--- If a call to await() times out or a thread blocked in await() is interrupted, then the barrier is considered broken and all outstanding calls to .await() terminate with BrokenBarrierException. 
--- If the barrier is successfully passed, .await() returns a unique arrival index for each thread, which can be used to "ELECT" a leader that takes some special action in the next iteration. 
---- CyclicBarrier also lets you pass a barrier action to the constructor; this is a Runnable that is executed (in one of the subtask threads) when the barrier is successfully passed but before the blocked threads are released. Выполнится в последнем потоке, который вызвал Barrier.await().

********** When using a CyclicBarrier, the assumption is that you specify the number of waiting threads that trigger the barrier. If you specify 5, you must have at least 5 threads to call await().

When using a CountDownLatch, you specify the number of calls to countDown() that will result in all waiting threads being released. 
This means that you can use a CountDownLatch with only a single thread.

!!! The CyclicBarrier uses an ALL-OR-NONE breakage model for failed synchronization attempts: 
If a thread leaves a barrier point prematurely(преждевременно) because of interruption, failure, or timeout, ALL OTHER THREADS waiting at that barrier point will also leave abnormally via BrokenBarrierException (or InterruptedException if they too were interrupted at about the same time). 
************

CellularAutomata.java
https://bitstorm.org/gameoflife/

Another form of barrier is EXCHANGER - a TWO-PARTY BARRIER in which the parties exchange data at the barrier point.

НАПРИМЕР:
 * Есть два грузовика: один едет из пункта A в пункт D, другой из пункта B в пункт С.
 * Из пунктов A и B нужно доставить посылки в пункты C и D.
 * Дороги AD и BC пересекаются в пункте E.
 * Грузовики должны встретиться в пункте E и обменяться соответствующими посылками.

- Exchangers are useful when the parties perform asymmetric activities, for example when one thread fills a buffer with data and the other thread consumes the data from the buffer; 
these threads could use an Exchanger to meet and exchange a full buffer for an empty one. 
When two threads exchange objects via an Exchanger, the exchange constitutes a safe publication of both objects to the other party.
The timing of the exchange depends on the responsiveness requirements of the application.
---------------------------------------------------

*** 5.6 Building an efficient, scalable result cache ***
we'd like to create a Computable wrapper that remembers the results of previous computations and encapsulates the caching process. (This technique is known as MEMOIZATION.)
** КАКИЕ ПРОБЛЕМЫ С КЭШЕМ И КАК ИХ РЕШАТЬ ??
*1* Memoizer1 - он плохой, потому что:

*2* Memoizer2 in Listing 5.17 improves on the awful concurrent behavior of Memoizer1 by replacing the HashMap with a ConcurrentHashMap. Since ConcurrentHashMap is thread-safe, there is no need to synchronize when accessing the backing Map, thus eliminating the serialization induced by synchronizing compute in Memoizer1.
But it still has some defects as a cache — there is a window of vulnerability in which two threads calling compute at the same time could end up computing the same value.
(The problem with Memoizer2 is that if one thread starts an expensive computation, other threads are NOT aware that the computation is in progress and so may start the same computation).

*3* Для этого надо дать понять, что такая-то задача уже выполняется, и второму потоку с такой же задачей надо лишь подождать результатов и получить их. Поэтому здесь надо ис-ть FutureTask.
--FutureTask represents a computational process that may or may not already have completed. FutureTask.get() returns the result of the computation immediately if it is available; 
otherwise it blocks until the result has been computed and then returns it.
--Memoizer3 first checks to see if the appropriate calculation has been started (as opposed to finished, as in Memoizer2). If not, it creates a FutureTask, registers it in the Map, and starts the computation; otherwise it waits for the result of the existing computation. The result might be available immediately or might be in the process of being computed — but this is transparent to the caller of Future.get().

*4* there is still a small window of vulnerability(уязвимость; ранимость) in which two threads might compute the same value. This window is far smaller than in Memoizer2, but because the if block in compute is still a NONATOMIC CHECK-THEN-ACT sequence, it is possible for two threads to call compute with the same value at roughly the same time, both see that the cache does not contain the desired value, and both start the computation. This unlucky timing is illustrated in Figure 5.4.

Memoizer in Listing 5.19 takes advantage of the atomic putIfAbsent() method of ConcurrentMap, closing the window of vulnerability in Memoizer3.

-- Caching a Future instead of a value creates the POSSIBILITY of [1] CACHE POLLUTION: (когда кэш в невалидном состоянии).
if a computation is cancelled or fails, future attempts to compute the result will also indicate cancellation or failure. 
To avoid this, Memoizer removes the Future from the cache if it detects that the computation was cancelled; 
it might also be desirable to remove the Future upon detecting a RuntimeException if the computation might succeed on a future attempt.

-- Memoizer also does not address [2] CACHE EXPIRATION, but this could be accomplished by using a subclass of FutureTask that associates an expiration time with each result and periodically scanning the cache for expired entries. 
(Similarly, it does not address [3] CACHE EVICTION, where old entries are removed to make room for new ones so that the cache does not consume too much memory.)

Итого с кэшэм надо также продумать следующие моменты:
[1] CACHE POLLUTION
[2] CACHE EXPIRATION
[3] CACHE EVICTION

что такое кэш - переиспользование результатов вычисления/загруженных данных/результатов запросов (tcp / db).
Что должно быть у идеального кэша:
1) не считать одно и то же дважды (гарантировать что каждое зн-е будет вычислено только 1 раз)
2) в многопоточном приложении -- распределять вычисления на много потоков, если в 1 уже вычисляется нужное зн-е то другие кому надо не запускают новое, а ждут результатов первого.
3) CACHE EXPIRATION - Удалять истёкшие по валидности значения(либо обновлять эти значения). Например, курс валют каждый день.
4) CACHE EVICTION - освобождать кэш от старых [может быть неиспользуемых значений]. чтобы кэш не разрастался бесконтрольно.
5) CACHE POLLUTION - когда кэш в невалидном состоянии, надо не допускать такого.