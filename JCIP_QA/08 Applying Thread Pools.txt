Chapter 8 -- Applying Thread Pools -- 167-188 (21стр)
*** 8.1 Implicit couplings between tasks and execution policies ***
Executor framework decouples task submission from task execution.
Не все задачи подходят\совместимы с execution policies.

Специфическая политика выполнения нужна для:
1) DEPENDENT TASKS.
  The most well behaved tasks are independent: those that do not depend on the timing, results, or side effects of other tasks. When executing independent tasks in a thread pool, you can freely vary the pool size and configuration without affecting anything but performance.
  При зависимых задачах появляются доп. ограничения (constraints on the execution policy), которыми надо аккуратно управлять, чтобы избежать liveness problems.

2) Tasks that EXPLOIT(пользоваться, использовать) THREAD CONFINEMENT. 
  Single-threaded executors выполняют задачи последовательно.
  Objects can be confined to the task thread, thus enabling tasks designed to run in that thread to access those objects without synchronization, even if those resources are not thread-safe. 
  Если потом сменить Single-threaded executor на другой то потоко-безопасность не гарантируется.

3) Response-time-sensitive tasks.
  Например, GUI applications. 
  Поэтому долгие задачи лучше НЕ запускать в Single-threaded executor или в том, где мало потоков, т.к. это увеличить время ответа приложения. Пользователь может не дождаться и решить что прога повисла или сломалась.

4) Tasks that use ThreadLocal.
   ThreadLocal allows each thread to have its own private "version" of a variable. However, executors are free to reuse threads as they see fit. 
   The standard Executor implementations may reap idle threads when demand is low and add new ones when demand is high, and also replace a worker thread with a fresh one if an unchecked exception is thrown from a task. 
   ThreadLocal makes sense to use in pool threads only if the thread-local value has a lifetime that is bounded by that of a task; Thread-Local should not be used in pool threads to communicate values between tasks.

Thread pools work best when tasks are HOMOGENEOUS and INDEPENDENT. 
Mixing long-running and short-running tasks risks "clogging" the pool unless it is very large; submitting tasks that depend on other tasks risks deadlock unless the pool is unbounded.

Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; 
tasks that exploit thread confinement require sequential execution.
Поэтому в документации надо описать какую политику выполнения должны поддерживать задачи, чтобы программеры не попутали и не сломали неверной реализацией работу программы.

*** 8.1.1 Thread STARVATION deadlock ***
If tasks that depend on other tasks execute in a thread pool, they can deadlock.
Если задача зависит от результатов выполнения другой задачи -- то потенциально может случиться дэдлок.
Например, в single-threaded executor 2я задачи никогда не выполниться раньше 1й, которая запустила 2ю и ждёт результатов её выполнения.
Но такое же может случиться и в других executor, даже когда много потоков, но если они ждут выполнения других потоков, который застряли в ожидании в очереди.
Это называется THREAD STARVATION DEADLOCK, and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough. 

ThreadDeadlock 
RenderPageTask{} submits two additional tasks to the Executor to fetch the page header and footer, renders the page body, waits for the results of the header and footer tasks, and then combines the header, body, and footer into the finished page. With a SINGLE-THREADED executor, ThreadDeadlock WILL ALWAYS DEADLOCK. 
А также tasks координирующие своё выполнение через barrier could also cause thread starvation deadlock if the pool is not big enough.

В общем, если запускать зависимые задачи, надо задокументировать какой размер пула для них необходим и достаточен, чтобы не возник Thread STARVATION deadlock.
А кроме этого ещё могут быть дополнительные ограничения на другие ресурсы.[например, если есть только 10 подключений к БД, то сколько бы не было потоков, максимум сможет выполняться только 10 задач одновременно].

*** 8.1.2    Long-running tasks ***
Если есть много долгих задач, то в итоге и короткие задачи могут выполняться долго, т.к. пул будет забит выполняющимися долгими задачами. Все остальные будут ждать их окончания.
Чтобы этого избежать, можно испльзовать ожидания с таймаутами (use timed resource waits instead of unbounded waits).
Все блокирующие методы в библиотеке имеют параметр таймаут. И по истечению его можно либо остановить задачу, либо запустить ещё позже. Это освободит ресурсы, чтобы другие потоки\задачи могли попробовать сделать прогресс вперёд.

If a thread pool is frequently full of blocked tasks, this may also be a sign that the pool is too small.

*** 8.2    Sizing thread pools ***
The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting Runtime.availableProcessors().

Плохо:
Слишком большой пул -- т.к. будет борьба за ресурсы(ЦПУ, память), ресурсов будет не хватать.
Плохо:
Слишком маленький пул -- т.к. будет маленькая пропускная способность, и процессор будет простаивать.

To size a thread pool properly, you need to understand your computing environment, your resource budget, and the nature of your tasks. 
How many processors does the deployment system have? How much memory? Do tasks perform mostly computation, I/O, or some combination? Do they require a scarce resource, such as a JDBC connection? If you have different categories of tasks with very different behaviors, consider using multiple thread pools so each can be tuned according to its workload.

Если есть N цпу, то оптимально сделать N+1 поток, т.к. (Even compute-intensive threads occasionally take a page fault or pause for some other reason, so an "extra" runnable thread prevents CPU cycles from going unused when this happens.) 
For tasks that also include I/O or other blocking operations, you want a larger pool, since not all of the threads will be schedulable at all times. 
In order to size the pool properly, you must estimate the ratio of waiting time to compute time for your tasks; this estimate need not be precise and can be obtained through profiling or instrumentation.

N_cpu = number of CPUs
U_cpu = target CPU utilization, 0 < Ucpu < 1 
W/C = Wait/Compute = ratio of wait time to compute time
The optimal pool size for keeping the processors at the desired utilization is:
N_threads = N_cpu * U_cpu * ( 1 + W/C )

Other resources that can contribute to sizing constraints are memory, file handles, socket handles, and database connections. Calculating pool size constraints for these types of resources is easier: just add up how much of that resource each task requires and divide that into the total quantity available. The result will be an upper bound on the pool size.

When tasks require a pooled resource such as database connections, thread pool size and resource pool size affect each other. If each task requires a connection, the effective size of the thread pool is limited by the connection pool size. 
Similarly, when the only consumers of connections are pool tasks, the effective size of the connection pool is limited by the thread pool size.

*** 8.3 Configuring ThreadPoolExecutor ***
ThreadPoolExecutor provides the base implementation for the executors..

*** 8.3.1 Thread creation and teardown(демонтаж) ***
Создание и демонтаж определяются параметрами:

1) core pool size == corePoolSize
 - the number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set
 - the implementation attempts to maintain the pool at this size even when there are no tasks to execute
 - and will not create more threads than this unless the work queue is full
 - When a ThreadPoolExecutor is initially created, the core threads are not started immediately but instead as tasks are submitted, unless you call prestartAllCoreThreads.

2) maximum pool size == maximumPoolSize
 - the maximum number of threads to allow in the pool
 - the upper bound on how many pool threads can be active at once.

3) keep-alive time == keepAliveTime
 - when the number of threads is greater than the core, this is the maximum time that excess(излишний, избыточный) idle threads will wait for new tasks before terminating.
 - A thread that has been idle for longer than the keep-alive time becomes a candidate for reaping and can be terminated if the current pool size exceeds the core size.

The newFixedThreadPool factory sets both the core pool size and the maximum pool size to the requested pool size, creating the effect of infinite timeout; 
the newCachedThreadPool factory sets the maximum pool size to Integer.MAX_VALUE and the core pool size to zero with a timeout of one minute, creating the effect of an infinitely expandable thread pool that will contract again when demand decreases.

-- Только в SynchronousQueue можно задать corePoolSize=0, чтобы все потоки прибились, если больше нет работы. С другими Queue это вызовет проблему, что не будут создаваться новые потоки и новая работа не будет выполняться:
  Developers are sometimes tempted to set the core size to zero so that the worker threads will eventually be torn down and therefore won't prevent the JVM from exiting, but this can cause some strange-seeming behavior in thread pools that don't use a SynchronousQueue for their work queue (as newCachedThreadPool does). 
  IF THE POOL IS ALREADY AT THE CORE SIZE, ThreadPoolExecutor CREATES A NEW THREAD ONLY IF THE WORK QUEUE IS FULL. 
  So tasks submitted to a thread pool with a work queue that has any capacity and a core size of zero will not execute until the queue fills up, which is usually not what is desired. 
  In Java 6, allowCoreThreadTimeOut() allows you to request that all pool threads be able to time out; enable this feature with a core size of zero if you want a bounded thread pool with a bounded work queue but still have all the threads torn down when there is no work to do.

*** 8.3.2 Managing queued tasks ***
Bounded thread pools limit the number of tasks that can be executed concurrently. (The single-threaded executors are a notable special case: they guarantee that no tasks will execute concurrently, offering the possibility of achieving thread safety through thread confinement.)

If the arrival rate for new requests exceeds the rate at which they can be handled, requests will still queue up. И это создаст доп. нагрузку на ресурсы и приложение. И чтобы избежать out of memory придётся просто прекращать обслуживать запросы, например.
 ...сглаживаем пик когда есть очередь.

ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution. 
There are three basic approaches to task queueing: 
1) UNBOUNDED queue 
2) BOUNDED queue 
3) SYNCHRONOUS handoff

The choice of queue interacts with other configuration parameters such as pool size.

newFixedThreadPool and newSingleThreadExecutor используют unbounded LinkedBlockingQueue. 

** какие ограниченные очереди а какие нет, примеры из java?
** что такое heap структура данных?

 -- A more stable resource management strategy is to use a bounded queue, such as an ArrayBlockingQueue or a bounded LinkedBlockingQueue or PriorityBlockingQueue. 
 Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. (There are a number of possible SATURATION [ˏsæʧə`reıʃ(ə)n] (насыщение, перегрузка\превышение имеющейся (доступной) пропускной способности) POLICIES for addressing this problem; see Section 8.3.3.)
   WITH A BOUNDED WORK QUEUE, THE QUEUE SIZE AND POOL SIZE MUST BE TUNED TOGETHER. 
   A large queue coupled with a small pool can help reduce memory usage, CPU usage, and context switching, at the cost of potentially constraining throughput.

 -- For very large or unbounded pools, you can also bypass queueing entirely and instead hand off tasks directly from producers to worker threads using a SynchronousQueue. 
 Для очень больших и безграничных пулов можно вообще пропустить "queueing" (т.е. сохранение в очередь), а можно сразу передавать задачи от производителя к рабочему потоку (который сразу же начнёт выполнять эту задачу) используя SynchronousQueue.

  A SynchronousQueue is NOT really A QUEUE at all, BUT a MECHANISM for managing handoffs between threads. 
  In order to put an element on a SynchronousQueue, another thread must already be waiting to accept the handoff. If no thread is waiting but the CURRENT POOL SIZE IS LESS THAN THE MAXIMUM, ThreadPoolExecutor creates a new thread; otherwise the task is rejected according to the saturation policy. 
  Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. 
  SynchronousQueue is a practical choice ONLY IF the pool is unbounded or if rejecting excess(излишний, избыточный) tasks is acceptable. 
  The newCachedThreadPool factory uses a SynchronousQueue.

Using a FIFO queue like LinkedBlockingQueue or ArrayBlockingQueue causes tasks to be started in the order in which they arrived. For more control over task execution order, you can use a PriorityBlockingQueue, which orders tasks according to priority. Priority can be defined by natural order (if tasks implement Comparable) or by a Comparator.

The newCachedThreadPool factory is a good default choice for an Executor, providing better queuing performance than a fixed thread pool.

Но Для веб-приложений разумно использовать fixed size thread pool, т.к. это предотвратит прилагу от уязвимости при перегрузке (vulnerable -- [`vʌln(ə)rəb(ə)l] - (уязвимый; ранимый) - to overload).

Для зависимых задач использование ограниченных пулов может привести к thread starvation deadlock. 
Поэтому для них надо использовать unbounded pool configuration like newCachedThreadPool.
ИЛИ же
Analternative configuration for tasks that submit other tasks and wait for their results is to use a bounded thread pool,a SynchronousQueue as the work queue, and the caller-runs saturation policy.

*** 8.3.3 Saturation policies ***
When a bounded work queue fills up, the saturation policy comes into play. 
The saturation policy for a ThreadPoolExecutor can be modified by calling setRejectedExecutionHandler(). 
The saturation policy is also used when a task is submitted to an Executor that has been shut down. 
Several implementations of RejectedExecutionHandler are provided, each implementing a different saturation policy: 

1) AbortPolicy == по дефолту
  The default policy, abort, causes execute to throw the unchecked RejectedExecutionException; the caller can catch this exception and implement its own overflow handling as it sees fit.

2) DiscardPolicy 
  The discard policy silently discards the newly submitted task if it cannot be queued for execution; 

3) DiscardOldestPolicy
  the discard-oldest policy discards the task that would otherwise be executed next and tries to resubmit the new task. 
  (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.)

4) CallerRunsPolicy
  The caller-runs policy implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. 
  It executes the newly submitted task not in a pool thread, but in the thread that calls execute.
  В этом случае тот поток, который давал задания сам начнёт их выполнять, при этом он перестанет посылать новые задачи, за это время потоки может быть разгребутся.
  Но в этом случае есть проблема, что сам главный поток тоже не сможет обрабатывать запросы, которые к нему приходят. Если это веб-сервер -- то запросы будут складироваться(добавляться в очередь) на уровне TCP. И потом уже TCP начнёт отказывать в обработке запросам. Т.е. при сверх-нагрузке перегрузка постепенно будет просачиваться от потоков в приложение, от него в TCP, и от TCP к клиенту.

По умолчанию нету такой политики, чтобы метод execute() блокировался, если очередь заполнилась. Можно сделать кастомный, с ис-ем семафора.
!!!  semaphore is bounding the number of tasks both currently executing and
awaiting execution.
см BoundedExecutor.java

*** 8.3.4 Thread factories ***
Whenever a thread pool needs to create a thread, it does so through a thread factory (see Listing 8.5). The default thread factory creates a new, nondaemon thread with no special configuration. 
Specifying a thread factory allows you to customize the configuration of pool threads. 

ThreadFactory has a single method, newThread() , that is called whenever a thread pool needs to create a new thread.

Можно создать свою custom thread factory, и в ней задать UncaughtExceptionHandler, или создавать instance of a custom Thread class.

If your application takes advantage of SECURITY POLICIES to grant permissions to particular codebases, you may want to use the privilegedThreadFactory() factory method in Executors to construct your thread factory. It creates pool threads that have the same permissions, AccessControlContext , and contextClassLoader as the thread creating the privilegedThreadFactory.
Otherwise, THREADS created by the thread pool INHERIT PERMISSIONS FROM whatever CLIENT happens to be calling execute() or submit() at the time a new thread is needed, which could cause confusing security-related exceptions.

*** 8.3.5 Customizing ThreadPoolExecutor after construction *** 
Most of the options passed to the ThreadPoolExecutor constructors can also be modified after construction via setters (such as the core thread pool size, maximum thread pool size, keep-alive time, thread factory, and rejected execution handler). If the Executor is created through one of the factory methods in Executors (except newSingleThreadExecutor), you can cast the result to ThreadPoolExecutor to access the setters.
А чтобы нельзя было изменять эти свойства после создания пула -- можно обернуть Executor в unconfigurableExecutorService() метод. Так мы гарантируем что лошара не сломает логику.
Так сделано например для newSingleThreadExecutor().

*** 8.4 Extending ThreadPoolExecutor ***
1,2) The beforeExecute() and afterExecute() hooks are called in the thread that executes the task, and can be used for adding logging, timing, monitoring, or statistics gathering. 
- The afterExecute() hook is called whether the task completes by returning normally from run or by throwing an Exception. 
- If the task completes with an Error, afterExecute() is not called.
- If beforeExecute() throws a RuntimeException, the task is not executed and afterExecute() is not called.

3) The terminated() hook is called when the thread pool completes the shutdown process, after all tasks have finished and all worker threads have shut down. 
It can be used to release resources allocated by the Executor during its lifecycle, perform notification or logging, or finalize statistics gathering.

*** 8.4.1 Example: adding statistics to a thread pool *** 
Because execution hooks are called in the thread that executes the task, a value placed in a ThreadLocal by beforeExecute() can be retrieved by afterExecute().

*** 8.5 Parallelizing recursive algorithms ***
If we have a loop whose iterations are independent and we don't need to wait for all of them to complete before proceeding, we can use an Executor to transform a sequential loop into a parallel one.

Sequential loop iterations are suitable for parallelization when EACH ITERATION IS INDEPENDENT of the others AND the WORK DONE IN EACH ITERATION of the loop body is significant enough to offset the cost of managing a new task. (и работа в каждой итерации покроет затраты на создание новой задачи).

см TransformingSequential.java
A call to processInParallel() returns more quickly than a call to processSequentially() because it returns as soon as all the tasks are queued to the Executor, rather than waiting for them all to complete. 
- If you want to submit a set of tasks and wait for them all to complete, you can use ExecutorService.invokeAll(); 
- to retrieve the results as they become available, you can use a CompletionService, as in Renderer.java

When parallelRecursive() returns, each node in the tree has been visited (the traversal is still sequential: only the calls to compute are executed in parallel) and the computation for each node has been queued to the Executor. 
Callers of parallelRecursive() can wait for all the results by creating an Executor specific to the traversal and using shutdown and awaitTermination().

*** 8.5.1 Example: A puzzle framework ***
The concurrent approach also trades one form of limitation for another that might be more suitable to the problem domain. The sequential version performs a depth-first search, so the search is bounded by the available stack size. The concurrent version performs a breadth-first search and is therefore free of the stack size restriction (but can still run out of memory if the set of positions to be searched or already searched exceeds the available memory).

In order to stop searching when we find a solution, we need a way to determine whether any thread has found a solution yet. If we want to accept the first solution found, we also need to update the solution only if no other task has already found one. These requirements describe a sort of latch (see Section 5.5.1) and in particular, a result-bearing latch. 

ValueLatch.java provides a way to hold a value such that only the first call actually sets the value, callers can test whether it has been set, and callers can block waiting for it to be set.

Когда будет происходить RejectedExecutionHandler ?

the rejected execution handler should be set to discard submitted tasks. Then, all unfinished tasks eventually run to completion and any subsequent attempts to execute new tasks fail silently, allowing the executor to terminate. (If the tasks took longer to run, we might want to interrupt them instead of letting them finish.)

The sequential version terminated when it had exhausted the search space, but getting concurrent programs to terminate can sometimes be more difficult. One possible solution is to keep a count of active solver tasks and set the solution to null when the count drops to zero

Finding the solution may also take longer than we are willing to wait; there are several additional termination conditions we could impose on the solver. 
One is a time limit; this is easily done by implementing a timed getValue() in ValueLatch (which would use the timed version of await()), and shutting down the Executor and declaring failure if getValue() times out. 
Another is some sort of puzzle-specific metric such as searching only up to a certain number of positions. Or we can provide a cancellation mechanism and let the client make its own decision about when to stop searching.
